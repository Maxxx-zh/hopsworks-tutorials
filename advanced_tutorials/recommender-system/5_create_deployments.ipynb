{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üë®üèª‚Äçüè´ Create Deployment </span>\n",
    "\n",
    "In this notebook, you'll create a deployment for your recommendation system.\n",
    "\n",
    "**NOTE Currently the transformer scripts are not implemented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-23 22:47:54,648 INFO: Initializing external client\n",
      "2024-12-23 22:47:54,649 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-23 22:47:56,264 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1206436\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Hopsworks Model Registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "dataset_api = project.get_dataset_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üöÄ Ranking Model Deployment </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You start by deploying your ranking model. Since it is a CatBoost model you need to implement a `Predict` class that tells Hopsworks how to load the model and how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name: 'ranking_model', version: 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_model = mr.get_best_model(\n",
    "    name=\"ranking_model\", \n",
    "    metric=\"fscore\", \n",
    "    direction=\"max\",\n",
    ")\n",
    "ranking_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ranking_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ranking_transformer.py\n",
    "\n",
    "import logging\n",
    "\n",
    "import hopsworks\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        # Connect to Hopsworks\n",
    "        project = hopsworks.login()\n",
    "        self.fs = project.get_feature_store()\n",
    "\n",
    "        # Retrieve 'transactions' feature group.\n",
    "        self.transactions_fg = self.fs.get_feature_group(\"transactions\", 1)\n",
    "\n",
    "        # Retrieve the 'articles' feature view\n",
    "        self.articles_fv = self.fs.get_feature_view(\n",
    "            name=\"articles\",\n",
    "            version=1,\n",
    "        )\n",
    "\n",
    "        # Get list of feature names for articles\n",
    "        self.articles_features = [feat.name for feat in self.articles_fv.schema]\n",
    "\n",
    "        # Retrieve the 'customers' feature view\n",
    "        self.customer_fv = self.fs.get_feature_view(\n",
    "            name=\"customers\",\n",
    "            version=1,\n",
    "        )\n",
    "\n",
    "        self.customer_fv.init_serving(1)\n",
    "\n",
    "        # Retrieve the 'candidate_embeddings' feature view\n",
    "        self.candidate_index = self.fs.get_feature_view(\n",
    "            name=\"candidate_embeddings\",\n",
    "            version=1,\n",
    "        )\n",
    "\n",
    "        # Retrieve ranking model\n",
    "        mr = project.get_model_registry()\n",
    "        model = mr.get_model(\n",
    "            name=\"ranking_model\",\n",
    "            version=1,\n",
    "        )\n",
    "\n",
    "        self.ranking_fv = model.get_feature_view(init=False)\n",
    "        self.ranking_fv.init_batch_scoring(1)\n",
    "\n",
    "        # Get the names of features expected by the ranking model\n",
    "        self.ranking_model_feature_names = [\n",
    "            feature.name \n",
    "            for feature \n",
    "            in self.ranking_fv.schema \n",
    "            if feature.name != 'label'\n",
    "        ]\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        # Extract the input instance\n",
    "        inputs = inputs[\"instances\"][0]\n",
    "\n",
    "        # Extract customer_id from inputs\n",
    "        customer_id = inputs[\"customer_id\"]\n",
    "\n",
    "        # Search for candidate items\n",
    "        neighbors = self.candidate_index.find_neighbors(\n",
    "            inputs[\"query_emb\"],\n",
    "            k=100,\n",
    "        )\n",
    "        neighbors = [neighbor[0] for neighbor in neighbors]\n",
    "\n",
    "        # Get IDs of items already bought by the customer\n",
    "        already_bought_items_ids = (\n",
    "            self.transactions_fg.select(\"article_id\").filter(self.transactions_fg.customer_id==customer_id).read(dataframe_type=\"pandas\").values.reshape(-1).tolist()\n",
    "        )\n",
    "\n",
    "        # Filter candidate items to exclude those already bought by the customer\n",
    "        item_id_list = [\n",
    "            str(item_id)\n",
    "            for item_id in neighbors\n",
    "            if str(item_id) not in already_bought_items_ids\n",
    "        ]\n",
    "        item_id_df = pd.DataFrame({\"article_id\": item_id_list})\n",
    "\n",
    "        # Retrieve Article data for candidate items\n",
    "        articles_data = [\n",
    "            self.articles_fv.get_feature_vector({\"article_id\": item_id})\n",
    "            for item_id in item_id_list\n",
    "        ]\n",
    "\n",
    "        logging.info(\"‚úÖ Articles Data Retrieved!\")\n",
    "\n",
    "        articles_df = pd.DataFrame(\n",
    "            data=articles_data,\n",
    "            columns=self.articles_features,\n",
    "        )\n",
    "\n",
    "        # Join candidate items with their features\n",
    "        ranking_model_inputs = item_id_df.merge(\n",
    "            articles_df,\n",
    "            on=\"article_id\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "\n",
    "        logging.info(\"‚úÖ Inputs are almost ready!\")\n",
    "\n",
    "        # Add customer features\n",
    "        customer_features = self.customer_fv.get_feature_vector(\n",
    "                {\"customer_id\": customer_id},\n",
    "                return_type=\"pandas\",\n",
    "            )\n",
    "\n",
    "        ranking_model_inputs[\"age\"] = customer_features.age.values[0]\n",
    "        ranking_model_inputs[\"month_sin\"] = inputs[\"month_sin\"]\n",
    "        ranking_model_inputs[\"month_cos\"] = inputs[\"month_cos\"]\n",
    "\n",
    "        # Select only the features required by the ranking model\n",
    "        ranking_model_inputs = ranking_model_inputs[self.ranking_model_feature_names]\n",
    "\n",
    "        logging.info(\"‚úÖ Inputs are ready!\")\n",
    "\n",
    "        return {\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"ranking_features\": ranking_model_inputs.values.tolist(),\n",
    "                    \"article_ids\": item_id_list,\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        logging.info(\"‚úÖ Predictions are ready!\")\n",
    "\n",
    "        # Merge prediction scores and corresponding article IDs into a list of tuples\n",
    "        ranking = list(zip(outputs[\"scores\"], outputs[\"article_ids\"]))\n",
    "\n",
    "        # Sort the ranking list by score in descending order\n",
    "        ranking.sort(reverse=True)\n",
    "\n",
    "        # Return the sorted ranking list\n",
    "        return {\n",
    "            \"ranking\": ranking,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30def5ce4b9a45f99e4615b59133a291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/4451 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy transformer file into Hopsworks File System \n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"ranking_transformer.py\",    # File name to be uploaded\n",
    "    \"Resources\",                 # Destination directory in Hopsworks File System \n",
    "    overwrite=True,              # Overwrite the file if it already exists\n",
    ") \n",
    "\n",
    "# Construct the path to the uploaded transformer script\n",
    "transformer_script_path = os.path.join(\n",
    "    \"/Projects\",                 # Root directory for projects in Hopsworks\n",
    "    project.name,                # Name of the current project\n",
    "    uploaded_file_path,          # Path to the uploaded file within the project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ranking_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ranking_predictor.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "class Predict(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = joblib.load(os.environ[\"MODEL_FILES_PATH\"] + \"/ranking_model.pkl\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        logging.info(f\"‚úÖ Inputs: {inputs}\")\n",
    "        \n",
    "        # Extract ranking features and article IDs from the inputs\n",
    "        features = inputs[0].pop(\"ranking_features\")\n",
    "        article_ids = inputs[0].pop(\"article_ids\")\n",
    "        \n",
    "        # Log the extracted features\n",
    "        logging.info(\"predict -> \" + str(features))\n",
    "        \n",
    "        # Log the extracted article ids\n",
    "        logging.info(f'Article IDs: {article_ids}')\n",
    "        \n",
    "        logging.info(f\"ü¶Ö Predicting...\")\n",
    "\n",
    "        # Predict probabilities for the positive class\n",
    "        scores = self.model.predict_proba(features).tolist()\n",
    "        \n",
    "        # Get scores of positive class\n",
    "        scores = np.asarray(scores)[:,1].tolist() \n",
    "\n",
    "        # Return the predicted scores along with the corresponding article IDs\n",
    "        return {\n",
    "            \"scores\": scores, \n",
    "            \"article_ids\": article_ids,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16887c9b56e74c759ac4be16ebcc333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/1114 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload predictor file to Hopsworks\n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"ranking_predictor.py\", \n",
    "    \"Resources\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded script\n",
    "predictor_script_path = os.path.join(\n",
    "    \"/Projects\",\n",
    "    project.name,\n",
    "    uploaded_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, you can finally deploy your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/1206436/deployments/353316\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "ranking_deployment_name = \"rankingdeployment\"\n",
    "\n",
    "# Define transformer\n",
    "ranking_transformer=Transformer(\n",
    "    script_file=transformer_script_path,\n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy ranking model\n",
    "ranking_deployment = ranking_model.deploy(\n",
    "    name=ranking_deployment_name,\n",
    "    description=\"Deployment that search for item candidates and scores them based on customer metadata\",\n",
    "    script_file=predictor_script_path,\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=ranking_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f902081e1a24006bc63c4ab549d5437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "# Start the deployment\n",
    "ranking_deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://c.app.hopsworks.ai:443/p/1206436/deployments/353316\n",
      "\n",
      "DeployableComponentLogs(instance_name: 'rankingdeployment-transformer-00001-deployment-58f8bd6496-dxcxc', date: datetime.datetime(2024, 12, 23, 22, 48, 33, 727153)) \n",
      "INFO:root:Loading component module...\n",
      "INFO:root:[TransformerModel] Initializing transformer for model: rankingdeployment\n",
      "INFO:root:[HopsworksModel] Initializing for model: rankingdeployment\n",
      "INFO:hsfs.engine.python:Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai/p/1206436\n",
      "... execution time: 10.083744 seconds\n",
      "INFO:root:Starting KServe server...\n",
      "2024-12-23 20:48:27.856 7 kserve INFO [model_server.py:register_model():363] Registering model: rankingdeployment\n",
      "2024-12-23 20:48:27.856 7 kserve INFO [model_server.py:start():298] Setting max asyncio worker threads as 12\n",
      "2024-12-23 20:48:27.856 7 kserve INFO [model_server.py:_serve_rest():244] Starting uvicorn with 1 workers\n",
      "WARNING:py.warnings:DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions\n",
      "\n",
      "WARNING:py.warnings:DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated\n",
      "\n",
      "2024-12-23 20:48:27.893 uvicorn.error INFO:     Started server process [7]\n",
      "2024-12-23 20:48:27.893 uvicorn.error INFO:     Waiting for application startup.\n",
      "2024-12-23 20:48:27.895 7 kserve INFO [server.py:start():68] Starting gRPC server on [::]:8081\n",
      "2024-12-23 20:48:27.895 uvicorn.error INFO:     Application startup complete.\n",
      "2024-12-23 20:48:27.895 uvicorn.error INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check logs in case of failure\n",
    "ranking_deployment.get_logs(component=\"transformer\", tail=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_recommendations(ranked_candidates, k=3):\n",
    "    return [candidate[-1] for candidate in ranked_candidates[\"ranking\"][:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['725954002', '721107001', '673281020']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a test input example\n",
    "test_ranking_input = [\n",
    "        {\n",
    "            \"customer_id\": \"d327d0ad9e30085a436933dfbb7f77cf42e38447993a078ed35d93e3fd350ecf\",\n",
    "            \"month_sin\": 1.2246467991473532e-16,\n",
    "            \"query_emb\": [\n",
    "                0.214135289,\n",
    "                0.571055949,\n",
    "                0.330709577,\n",
    "                -0.225899458,\n",
    "                -0.308674961,\n",
    "                -0.0115124583,\n",
    "                0.0730511621,\n",
    "                -0.495835781,\n",
    "                0.625569344,\n",
    "                -0.0438038409,\n",
    "                0.263472944,\n",
    "                -0.58485353,\n",
    "                -0.307070434,\n",
    "                0.0414443575,\n",
    "                -0.321789205,\n",
    "                0.966559,\n",
    "            ],\n",
    "            \"month_cos\": -1.0,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Test ranking deployment\n",
    "ranked_candidates = ranking_deployment.predict(inputs=test_ranking_input)\n",
    "\n",
    "# Retrieve article ids of the top recommended items\n",
    "recommendations = get_top_recommendations(ranked_candidates[\"predictions\"], k=3)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://c.app.hopsworks.ai:443/p/1206436/deployments/353316\n",
      "\n",
      "DeployableComponentLogs(instance_name: 'rankingdeployment-transformer-00001-deployment-58f8bd6496-dxcxc', date: datetime.datetime(2024, 12, 23, 22, 48, 38, 90500)) \n",
      "INFO:root:Loading component module...\n",
      "INFO:root:[TransformerModel] Initializing transformer for model: rankingdeployment\n",
      "INFO:root:[HopsworksModel] Initializing for model: rankingdeployment\n",
      "INFO:hsfs.engine.python:Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai/p/1206436\n",
      "... execution time: 10.083744 seconds\n",
      "INFO:root:Starting KServe server...\n",
      "2024-12-23 20:48:27.856 7 kserve INFO [model_server.py:register_model():363] Registering model: rankingdeployment\n",
      "2024-12-23 20:48:27.856 7 kserve INFO [model_server.py:start():298] Setting max asyncio worker threads as 12\n",
      "2024-12-23 20:48:27.856 7 kserve INFO [model_server.py:_serve_rest():244] Starting uvicorn with 1 workers\n",
      "WARNING:py.warnings:DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions\n",
      "\n",
      "WARNING:py.warnings:DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated\n",
      "\n",
      "2024-12-23 20:48:27.893 uvicorn.error INFO:     Started server process [7]\n",
      "2024-12-23 20:48:27.893 uvicorn.error INFO:     Waiting for application startup.\n",
      "2024-12-23 20:48:27.895 7 kserve INFO [server.py:start():68] Starting gRPC server on [::]:8081\n",
      "2024-12-23 20:48:27.895 uvicorn.error INFO:     Application startup complete.\n",
      "2024-12-23 20:48:27.895 uvicorn.error INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "INFO:root:Received request via 'v1 protocol'\n",
      "WARNING:py.warnings:VersionWarning: No training dataset version was provided to initialise serving. Defaulting to version 1.\n",
      "\n",
      "WARNING:py.warnings:DeprecationWarning: HTTPResponse.getheaders() is deprecated and will be removed in urllib3 v2.1.0. Instead access HTTPResponse.headers directly.\n",
      "\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.54s) \n",
      "WARNING:py.warnings:VersionWarning: No training dataset version was provided to initialise serving. Defaulting to version 1.\n",
      "\n",
      "INFO:root:‚úÖ Articles Data Retrieved!\n",
      "INFO:root:‚úÖ Inputs are almost ready!\n",
      "INFO:root:‚úÖ Inputs are ready!\n",
      "INFO:httpx:HTTP Request: POST http://rankingdeployment-predictor.tutorials/v1/models/rankingdeployment:predict \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Received response via 'v1 protocol'\n",
      "INFO:root:‚úÖ Predictions are ready!\n",
      "2024-12-23 20:48:37.647 kserve.trace requestId: abaafa9a-77d1-4f7c-94fa-a41ef4246ae4, preprocess_ms: 3587.480068207, explain_ms: 0, predict_ms: 41.852712631, postprocess_ms: 0.10228157\n",
      "2024-12-23 20:48:37.647 uvicorn.access INFO:     10.2.3.34:0 7 - \"POST /v1/models/rankingdeployment%3Apredict HTTP/1.1\" 200 OK\n",
      "2024-12-23 20:48:37.648 kserve.trace kserve.io.kserve.protocol.rest.v1_endpoints.predict: 3.6306967735290527 ['http_status:200', 'http_method:POST', 'time:wall']\n",
      "2024-12-23 20:48:37.648 kserve.trace kserve.io.kserve.protocol.rest.v1_endpoints.predict: 0.1576089999999999 ['http_status:200', 'http_method:POST', 'time:cpu']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check logs in case of failure\n",
    "ranking_deployment.get_logs(component=\"transformer\",tail=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üöÄ Query Model Deployment </span>\n",
    "\n",
    "Next, you'll deploy your query model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the 'query_model' from the Model Registry\n",
    "query_model = mr.get_model(\n",
    "    name=\"query_model\",\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting querymodel_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile querymodel_transformer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import hopsworks\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self):            \n",
    "        # Connect to the Hopsworks\n",
    "        project = hopsworks.login()\n",
    "        ms = project.get_model_serving()\n",
    "    \n",
    "        # Retrieve the 'customers' feature view\n",
    "        fs = project.get_feature_store()\n",
    "        self.customer_fv = fs.get_feature_view(\n",
    "            name=\"customers\", \n",
    "            version=1,\n",
    "        )\n",
    "        \n",
    "        # Retrieve  the \"ranking\" feature view and initialize the batch scoring server.\n",
    "        self.ranking_fv = fs.get_feature_view(name=\"ranking\", version=1)\n",
    "        self.ranking_fv.init_batch_scoring(1)\n",
    "        \n",
    "        # Retrieve the ranking deployment \n",
    "        self.ranking_server = ms.get_deployment(\"rankingdeployment\")\n",
    "        \n",
    "        \n",
    "    def preprocess(self, inputs):\n",
    "        # Check if the input data contains a key named \"instances\"\n",
    "        # and extract the actual data if present\n",
    "        inputs = inputs[\"instances\"] if \"instances\" in inputs else inputs\n",
    "        inputs = inputs[0]\n",
    "        \n",
    "        # Extract customer_id and transaction_date from the inputs\n",
    "        customer_id = inputs[\"customer_id\"]\n",
    "        transaction_date = inputs[\"transaction_date\"]\n",
    "        \n",
    "        # Extract month from the transaction_date\n",
    "        month_of_purchase = datetime.fromisoformat(inputs.pop(\"transaction_date\"))\n",
    "\n",
    "        # Get customer features\n",
    "        customer_features = self.customer_fv.get_feature_vector(\n",
    "            {\"customer_id\": customer_id},\n",
    "            return_type=\"pandas\",\n",
    "        )\n",
    "        \n",
    "        # Enrich inputs with customer age\n",
    "        inputs[\"age\"] = customer_features.age.values[0]  \n",
    "        \n",
    "        # Calculate the sine and cosine of the month_of_purchase\n",
    "        month_of_purchase = datetime.strptime(\n",
    "            transaction_date, \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    "        ).month\n",
    "\n",
    "        # Calculate the sine and cosine components for the month_of_purchase using on-demand transformation present in \"ranking\" feature view.\n",
    "        feature_vector = self.ranking_fv._batch_scoring_server.compute_on_demand_features(\n",
    "            feature_vectors=pd.DataFrame([inputs]), request_parameters={\"month\": month_of_purchase}\n",
    "        ).to_dict(orient=\"records\")[0]\n",
    "\n",
    "        inputs[\"month_sin\"] = feature_vector[\"month_sin\"]\n",
    "        inputs[\"month_cos\"] = feature_vector[\"month_cos\"]\n",
    "\n",
    "        return {\"instances\": [inputs]}\n",
    "    \n",
    "    def postprocess(self, outputs):\n",
    "        # Return ordered ranking predictions\n",
    "        return self.ranking_server.predict(inputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95db523bfe94d1fa5b7de8b9f701cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/2583 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy transformer file into Hopsworks File System\n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"querymodel_transformer.py\", \n",
    "    \"Models\", \n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded script\n",
    "transformer_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    uploaded_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/1206436/deployments/353317\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "query_model_deployment_name = \"querydeployment\"\n",
    "\n",
    "# Define transformer\n",
    "query_model_transformer=Transformer(\n",
    "    script_file=transformer_script_path, \n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy the query model\n",
    "query_model_deployment = query_model.deploy(\n",
    "    name=query_model_deployment_name,\n",
    "    description=\"Deployment that generates query embeddings from customer and item features using the query model\",\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=query_model_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have registered your deployment. To start it up you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d388d0783642a0b726c7b708887654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "# Start the deployment\n",
    "query_model_deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs in case of failure\n",
    "# query_model_deployment.get_logs(component=\"transformer\", tail=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"customer_id\": \"d327d0ad9e30085a436933dfbb7f77cf42e38447993a078ed35d93e3fd350ecf\",\n",
    "        \"transaction_date\": \"2022-11-15T12:16:25.330916\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: http://15.235.46.163/v1/models/querydeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"error\":\"AssertionError : \"}', error code: , error msg: , user msg: \n\n Check the model server logs by using `.get_logs()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ranked_candidates \u001b[38;5;241m=\u001b[39m query_model_deployment\u001b[38;5;241m.\u001b[39mpredict(inputs\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Retrieve article ids of the top recommended items\u001b[39;00m\n\u001b[1;32m      4\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_top_recommendations(ranked_candidates[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m], k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorials40/lib/python3.11/site-packages/hsml/deployment.py:213\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    177\u001b[0m     data: Union[Dict, InferInput] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     inputs: Union[List, Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m ):\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        `dict`. Inference response.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serving_engine\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m, data, inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorials40/lib/python3.11/site-packages/hsml/engine/serving_engine.py:596\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m    593\u001b[0m re\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    594\u001b[0m     re\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the model server logs by using `.get_logs()`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    595\u001b[0m )\n\u001b[0;32m--> 596\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m re\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorials40/lib/python3.11/site-packages/hsml/engine/serving_engine.py:580\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    578\u001b[0m through_hopsworks \u001b[38;5;241m=\u001b[39m serving_tool \u001b[38;5;241m!=\u001b[39m PREDICTOR\u001b[38;5;241m.\u001b[39mSERVING_TOOL_KSERVE\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serving_api\u001b[38;5;241m.\u001b[39msend_inference_request(\n\u001b[1;32m    581\u001b[0m         deployment_instance, payload, through_hopsworks\n\u001b[1;32m    582\u001b[0m     )\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    585\u001b[0m         re\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m RestAPIError\u001b[38;5;241m.\u001b[39mSTATUS_CODE_NOT_FOUND\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m re\u001b[38;5;241m.\u001b[39merror_code\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;241m==\u001b[39m ModelServingException\u001b[38;5;241m.\u001b[39mERROR_CODE_DEPLOYMENT_NOT_RUNNING\n\u001b[1;32m    588\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorials40/lib/python3.11/site-packages/hsml/core/serving_api.py:233\u001b[0m, in \u001b[0;36mServingApi.send_inference_request\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to a deployment with a certain id\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m:param deployment_instance: metadata object of the deployment to be used for the prediction\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m:rtype: Union[Dict, List[InferOutput]]\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deployment_instance\u001b[38;5;241m.\u001b[39mapi_protocol \u001b[38;5;241m==\u001b[39m IE\u001b[38;5;241m.\u001b[39mAPI_PROTOCOL_REST:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# REST protocol, use hopsworks or istio client\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_inference_request_via_rest_protocol(\n\u001b[1;32m    234\u001b[0m         deployment_instance, data, through_hopsworks\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# gRPC protocol, use the deployment grpc channel\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_inference_request_via_grpc_protocol(\n\u001b[1;32m    239\u001b[0m         deployment_instance, data\n\u001b[1;32m    240\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorials40/lib/python3.11/site-packages/hsml/core/serving_api.py:277\u001b[0m, in \u001b[0;36mServingApi._send_inference_request_via_rest_protocol\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    274\u001b[0m         with_base_path_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# send inference request\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _client\u001b[38;5;241m.\u001b[39m_send_request(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    279\u001b[0m     path_params,\n\u001b[1;32m    280\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    281\u001b[0m     data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(data),\n\u001b[1;32m    282\u001b[0m     with_base_path_params\u001b[38;5;241m=\u001b[39mwith_base_path_params,\n\u001b[1;32m    283\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorials40/lib/python3.11/site-packages/hopsworks_common/decorators.py:45\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(inst, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorials40/lib/python3.11/site-packages/hopsworks_common/client/base.py:186\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[0m\n\u001b[1;32m    181\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_token_expired(\n\u001b[1;32m    182\u001b[0m         request, stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTOKEN_EXPIRED_RETRY_INTERVAL, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: http://15.235.46.163/v1/models/querydeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"error\":\"AssertionError : \"}', error code: , error msg: , user msg: \n\n Check the model server logs by using `.get_logs()`"
     ]
    }
   ],
   "source": [
    "ranked_candidates = query_model_deployment.predict(inputs=data)\n",
    "\n",
    "# Retrieve article ids of the top recommended items\n",
    "recommendations = get_top_recommendations(ranked_candidates[\"predictions\"], k=3)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs in case of failure\n",
    "query_model_deployment.get_logs(component=\"transformer\",tail=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the deployment when you're not using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the ranking model deployment\n",
    "ranking_deployment.stop()\n",
    "\n",
    "# Stop the query model deployment\n",
    "query_model_deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the timer\n",
    "notebook_end_time = time.time()\n",
    "\n",
    "# Calculate and print the execution time\n",
    "notebook_execution_time = notebook_end_time - notebook_start_time\n",
    "print(f\"‚åõÔ∏è Notebook Execution time: {notebook_execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
