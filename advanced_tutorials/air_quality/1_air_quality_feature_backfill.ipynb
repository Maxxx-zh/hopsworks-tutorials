{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cd155d",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 01: Feature Backfill</span>\n",
    "\n",
    "**Note**: This tutorial does not support Google Colab.\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "\n",
    "1. Fetch historical data.\n",
    "2. Connect to the Hopsworks feature store.\n",
    "3. Create feature groups and insert them to the feature store.\n",
    "\n",
    "![tutorial-flow](../../images/01_featuregroups.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce71c0b2",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92001bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U 'hopsworks[python]' --quiet\n",
    "!pip install geopy folium streamlit-folium --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e974d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "from features import air_quality\n",
    "from functions.common_functions import convert_date_to_unix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d519dd",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üåç Representing the Target cities </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7a26b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the 'target_cities.json' file in read mode\n",
    "with open('target_cities.json') as json_file:\n",
    "    # Load the JSON data from the file into a Python dictionary\n",
    "    target_cities = json.load(json_file)\n",
    "\n",
    "# Now, 'target_cities' contains the data from the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8063796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folium map centered on the first location in the list\n",
    "my_map = folium.Map(location=[42.57, -44.092], zoom_start=3)\n",
    "\n",
    "for continent in target_cities:\n",
    "        for city_name, coords in target_cities[continent].items():\n",
    "            folium.CircleMarker(\n",
    "                location=coords,\n",
    "                popup=city_name,\n",
    "            ).add_to(my_map)\n",
    "#my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the map to an HTML file\n",
    "# my_map.save(\"map_all_target_cities.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d3674",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå´ Processing Air Quality data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081d3f2",
   "metadata": {},
   "source": [
    "### [üá™üá∫ EEA](https://discomap.eea.europa.eu/map/fme/AirQualityExport.htm)\n",
    "#### EEA means European Environmental Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986686f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EU Cities \n",
    "target_cities[\"EU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be358330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file from the specified URL into a pandas DataFrame\n",
    "df_eu = pd.read_csv(\"https://repo.hops.works/dev/davit/air_quality/backfill_pm2_5_eu.csv\")\n",
    "\n",
    "# Print the size of the 'df_eu' DataFrame (number of rows and columns)\n",
    "print(\"‚õ≥Ô∏è Size of this dataframe:\", df_eu.shape)\n",
    "\n",
    "# Check for missing values in the 'df_eu' DataFrame\n",
    "print(f'‚õ≥Ô∏è Missing Values: {df_eu.isna().sum().sum()}')\n",
    "\n",
    "# Display a random sample of three rows from the 'df_eu' DataFrame\n",
    "df_eu.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02141bd",
   "metadata": {},
   "source": [
    "### [üá∫üá∏ USEPA](https://aqs.epa.gov/aqsweb/documents/data_api.html#daily)\n",
    "#### USEPA means United States Environmental Protection Agency\n",
    "[Manual downloading](https://www.epa.gov/outdoor-air-quality-data/download-daily-data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c439b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# US Cities \n",
    "target_cities[\"US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429aebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the CSV file from the specified URL into a pandas DataFrame\n",
    "df_us = pd.read_csv(\"https://repo.hops.works/dev/davit/air_quality/backfill_pm2_5_us.csv\")\n",
    "\n",
    "# Print the size of the 'df_us' DataFrame (number of rows and columns)\n",
    "print(\"‚õ≥Ô∏è Size of this dataframe:\", df_us.shape)\n",
    "\n",
    "# Check for missing values in the 'df_us' DataFrame\n",
    "print(f'‚õ≥Ô∏è Missing Values: {df_us.isna().sum().sum()}')\n",
    "\n",
    "# Display a random sample of three rows from the 'df_us' DataFrame\n",
    "df_us.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7b660",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">üè¢ Processing special city - `Seattle`</span>\n",
    "#### We need different stations across the Seattle. \n",
    "I downloaded daily `PM2.5` data manually [here](https://www.epa.gov/outdoor-air-quality-data/download-daily-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401130e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cities[\"Seattle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac26217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the CSV file from the specified URL into a pandas DataFrame\n",
    "df_seattle = pd.read_csv(\"https://repo.hops.works/dev/davit/air_quality/backfill_pm2_5_seattle.csv\")\n",
    "\n",
    "# Print the size of the 'df_seattle' DataFrame (number of rows and columns)\n",
    "print(\"‚õ≥Ô∏è Size of this dataframe:\", df_seattle.shape)\n",
    "\n",
    "# Check for missing values in the 'df_seattle' DataFrame\n",
    "print(f'‚õ≥Ô∏è Missing Values: {df_seattle.isna().sum().sum()}')\n",
    "\n",
    "# Display a random sample of three rows\n",
    "df_seattle.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a6e68",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">üåü All together</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913087f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames df_eu, df_us, and df_seattle along the rows and reset the index\n",
    "df_air_quality = pd.concat(\n",
    "    [df_eu, df_us, df_seattle],\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Print the shape of the df_air_quality DataFrame\n",
    "print(f'‚õ≥Ô∏è DF shape: {df_air_quality.shape}')\n",
    "\n",
    "# Display a random sample of five rows from the df_air_quality DataFrame\n",
    "df_air_quality.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268791c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">üõ† Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7a97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the 'date' column in the df_air_quality DataFrame to datetime format\n",
    "df_air_quality['date'] = pd.to_datetime(df_air_quality['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45e480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply feature engineering to the df_air_quality DataFrame using the air_quality.feature_engineer_aq() function\n",
    "df_air_quality = air_quality.feature_engineer_aq(df_air_quality)\n",
    "\n",
    "# Drop rows with missing values in the df_air_quality DataFrame\n",
    "df_air_quality = df_air_quality.dropna()\n",
    "\n",
    "# Check and print the total number of missing values in the df_air_quality DataFrame\n",
    "df_air_quality.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8e1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the shape (number of rows and columns) of the df_air_quality DataFrame\n",
    "df_air_quality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c627429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve and display the column names of the df_air_quality DataFrame\n",
    "df_air_quality.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296b629",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style='color:#ff5f27'> üå¶ Loading Weather Data from [Open Meteo](https://open-meteo.com/en/docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file from the specified URL into a pandas DataFrame for weather data\n",
    "df_weather = pd.read_csv(\"https://repo.hops.works/dev/davit/air_quality/backfill_weather.csv\")\n",
    "\n",
    "# Display the first three rows of the df_weather DataFrame\n",
    "df_weather.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d4d7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e91c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the 'convert_date_to_unix' function to create a new 'unix_time' column in df_air_quality\n",
    "df_air_quality[\"unix_time\"] = pd.to_datetime(df_air_quality.date).apply(convert_date_to_unix)\n",
    "\n",
    "# Apply the 'convert_date_to_unix' function to create a new 'unix_time' column in df_weather\n",
    "df_weather[\"unix_time\"] = pd.to_datetime(df_weather.date).apply(convert_date_to_unix)\n",
    "\n",
    "# Convert the 'date' column in the df_air_quality DataFrame back to string format\n",
    "df_air_quality.date = df_air_quality.date.astype(str)\n",
    "\n",
    "# Convert the 'date' column in the df_weather DataFrame back to string format\n",
    "df_weather.date = df_weather.date.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f7eb5",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176991c",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™Ñ Creating Feature Groups</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370bbf0b",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üå´ Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a58bfc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get or create feature group\n",
    "air_quality_fg = fs.get_or_create_feature_group(\n",
    "    name='air_quality',\n",
    "    description='Air Quality characteristics of each day',\n",
    "    version=1,\n",
    "    primary_key=['unix_time','city_name'],\n",
    "    event_time=[\"unix_time\"],\n",
    ")   \n",
    "# Insert data\n",
    "air_quality_fg.insert(df_air_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cbe8aa",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üå¶ Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create feature group\n",
    "weather_fg = fs.get_or_create_feature_group(\n",
    "    name='weather',\n",
    "    description='Weather characteristics of each day',\n",
    "    version=1,\n",
    "    primary_key=['unix_time','city_name'],\n",
    "    event_time=[\"unix_time\"],\n",
    ") \n",
    "# Insert data\n",
    "weather_fg.insert(df_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5ffec",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 02: Feature Pipeline \n",
    " </span> \n",
    "\n",
    "In the following notebook you will parse data and insert it into Feature Groups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
