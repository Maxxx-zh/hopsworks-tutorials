{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93bddfdd",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 01: Load, Engineer & Connect</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\"> This is the first part of the AML tutorial. As part of this first module, you will work with data related to credit card transactions. \n",
    "The objective of this tutorial is to demonstrate how to work with the **Hopworks Feature Store** with a goal of training and deploying a model that can predict fraudulent transactions.</span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided into the following sections:** \n",
    "1. **Data Loading**: Load the data. \n",
    "2. **Feature Engineering**.\n",
    "2. **Hopsworks Feature Store Connection**.\n",
    "3. **Feature Groups Creation**: Create feature groups and upload them to the feature store.\n",
    "4. **Explore feature groups from the UI**.\n",
    "\n",
    "![tutorial-flow](../images/01_featuregroups.png)\n",
    "\n",
    "First of all we will load the data and do some feature engineering on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc0acc",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e94709c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 16:37:01.205018: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_ffn(hidden_units: list, dropout_rate: float, name: str = None) -> keras.Sequential:\n",
    "    \"\"\"\n",
    "    Create a feedforward neural network layer.\n",
    "\n",
    "    Parameters:\n",
    "    - hidden_units (list): List of integers specifying the number of units in each hidden layer.\n",
    "    - dropout_rate (float): Dropout rate for regularization.\n",
    "    - name (str): Name of the layer.\n",
    "\n",
    "    Returns:\n",
    "    keras.Sequential: Feedforward neural network layer.\n",
    "    \"\"\"\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n",
    "\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Layer.\n",
    "\n",
    "    Parameters:\n",
    "    - hidden_units (list): List of integers specifying the number of units in each hidden layer.\n",
    "    - dropout_rate (float): Dropout rate for regularization.\n",
    "    - aggregation_type (str): Type of aggregation for neighbor messages ('sum', 'mean', 'max').\n",
    "    - combination_type (str): Type of combination for node embeddings ('gated', 'gru', 'concat', 'add').\n",
    "    - normalize (bool): Flag to normalize node embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units: list,\n",
    "        dropout_rate: float = 0.2,\n",
    "        aggregation_type: str = \"mean\",\n",
    "        combination_type: str = \"concat\",\n",
    "        normalize: bool = False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Prepare neighbor messages.\n",
    "\n",
    "        Parameters:\n",
    "        - node_repesentations (tf.Tensor): Node representations.\n",
    "        - weights (tf.Tensor): Weights for neighbor messages.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: Prepared neighbor messages.\n",
    "        \"\"\"\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Aggregate neighbor messages.\n",
    "\n",
    "        Parameters:\n",
    "        - node_indices (tf.Tensor): Node indices.\n",
    "        - neighbour_messages (tf.Tensor): Neighbor messages.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: Aggregated messages.\n",
    "        \"\"\"\n",
    "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Update node embeddings.\n",
    "\n",
    "        Parameters:\n",
    "        - node_repesentations (tf.Tensor): Node representations.\n",
    "        - aggregated_messages (tf.Tensor): Aggregated neighbor messages.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: Updated node embeddings.\n",
    "        \"\"\"\n",
    "        if self.combination_type == \"gru\":\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Process inputs to produce node embeddings.\n",
    "\n",
    "        Parameters:\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: Node embeddings.\n",
    "        \"\"\"\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
    "        return self.update(node_repesentations, aggregated_messages)\n",
    "\n",
    "\n",
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Graph Neural Network Node Classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - graph_info: Tuple of node_features, edges, and edge_weights.\n",
    "    - hidden_units (list): List of integers specifying the number of units in each hidden layer.\n",
    "    - aggregation_type (str): Type of aggregation for neighbor messages ('sum', 'mean', 'max').\n",
    "    - combination_type (str): Type of combination for node embeddings ('gated', 'gru', 'concat', 'add').\n",
    "    - dropout_rate (float): Dropout rate for regularization.\n",
    "    - normalize (bool): Flag to normalize node embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info: tuple,\n",
    "        hidden_units: list,\n",
    "        aggregation_type: str = \"sum\",\n",
    "        combination_type: str = \"concat\",\n",
    "        dropout_rate: float = 0.2,\n",
    "        normalize: bool = True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNNodeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        self.compute_logits = layers.Dense(hidden_units[0], activation=tf.nn.tanh, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "\n",
    "        Parameters:\n",
    "        - input_node_indices (tf.Tensor): Input node indices.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: Predictions.\n",
    "        \"\"\"\n",
    "        x = self.preprocess(self.node_features)\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        x = x1 + x\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        x = x2 + x\n",
    "        x = self.postprocess(x)\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        return self.compute_logits(node_embeddings)\n",
    "\n",
    "\n",
    "def construct_graph(input_df: pd.DataFrame, data_party_labels: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Construct a graph and generate node embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - input_df (pd.DataFrame): Input transaction DataFrame.\n",
    "    - data_party_labels (pd.DataFrame): DataFrame containing party labels.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with keys 'id' and 'graph_embeddings'.\n",
    "    \"\"\"\n",
    "    sampled_party = data_party_labels[data_party_labels.id.isin(input_df.source) | (data_party_labels.id.isin(input_df.target))]\n",
    "    sampled_party = sampled_party[[\"id\", \"type\", \"is_sar\"]]\n",
    "\n",
    "    unique_ids = set(sampled_party.id.values)\n",
    "    id_dict = {idn: i for i, idn in enumerate(unique_ids)}\n",
    "\n",
    "    sampled_party['int_id'] = sampled_party['id'].apply(lambda x: id_dict[x])\n",
    "    input_df['source'] = input_df['source'].apply(lambda x: id_dict[x])\n",
    "    input_df['target'] = input_df['target'].apply(lambda x: id_dict[x])\n",
    "\n",
    "    feature_names = [\"type\"]\n",
    "    x_train = sampled_party.int_id.to_numpy()\n",
    "\n",
    "    edges = input_df[[\"source\", \"target\"]].to_numpy().T\n",
    "    edge_weights = tf.ones(shape=edges.shape[1])\n",
    "    node_features = tf.cast(\n",
    "        sampled_party.sort_values(\"id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    "    )\n",
    "    graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "    # Hyperparameters for graph embeddings model\n",
    "    hidden_units = [32, 32]\n",
    "    learning_rate = 0.01\n",
    "    dropout_rate = 0.5\n",
    "    num_epochs = 2\n",
    "    batch_size = 256\n",
    "\n",
    "    # Construct the model\n",
    "    model = GNNNodeClassifier(\n",
    "        graph_info=graph_info,\n",
    "        hidden_units=hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        name=\"gnn_model\",\n",
    "    )\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=x_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    graph_embeddings = list(model.predict(x_train).reshape(node_features.shape[0], hidden_units[0]))\n",
    "    return {\"id\": sampled_party.id.to_numpy(), \"graph_embeddings\": graph_embeddings}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8906dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_party_labels(data_transaction_labels: pd.DataFrame, data_party: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign SAR(Suspicious Activity Reports) labels to parties based on transaction data.\n",
    "\n",
    "    Parameters:\n",
    "    - data_transaction_labels (pd.DataFrame): DataFrame containing transaction labels, including SAR information.\n",
    "    - data_party (pd.DataFrame): DataFrame with party information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with party labels indicating SAR occurrences.\n",
    "    \"\"\"\n",
    "    alert_transactions = data_transaction_labels[data_transaction_labels.is_sar == 1]\n",
    "    alert_sources = alert_transactions[[\"source\", \"tran_timestamp\"]]\n",
    "    alert_sources.columns = [\"id\", \"tran_timestamp\"]\n",
    "    alert_targets = alert_transactions[[\"target\", \"tran_timestamp\"]]\n",
    "    alert_targets.columns = [\"id\", \"tran_timestamp\"]\n",
    "    sar_party = alert_sources.append(alert_targets, ignore_index=True)\n",
    "    sar_party.sort_values([\"id\", \"tran_timestamp\"], ascending=[False, True])\n",
    "\n",
    "    # Find the first occurrence of SAR per ID\n",
    "    sar_party = sar_party.iloc[[sar_party.id.eq(id).idxmax() for id in sar_party['id'].value_counts().index]]\n",
    "    sar_party = sar_party.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'id']).agg(monthly_count=('id', 'count'))\n",
    "    sar_party = sar_party.reset_index(level=[\"id\"])\n",
    "    sar_party = sar_party.reset_index(level=[\"tran_timestamp\"])\n",
    "    sar_party.drop([\"monthly_count\"], axis=1, inplace=True)\n",
    "\n",
    "    sar_party[\"is_sar\"] = sar_party[\"is_sar\"] = 1\n",
    "\n",
    "    party_labels = data_party.merge(sar_party, on=[\"id\"], how=\"left\")\n",
    "    party_labels.is_sar = party_labels.is_sar.map({1.0: 1, np.nan: 0})\n",
    "    max_time_stamp = datetime.datetime.utcfromtimestamp(int(max(data_transaction_labels.tran_timestamp.values)) / 1e9)\n",
    "    party_labels = party_labels.fillna(max_time_stamp)\n",
    "\n",
    "    return party_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011cd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_transaction_labels(data_transactions: pd.DataFrame, data_alert_transactions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge transaction data with alert transaction data to get labels indicating SAR occurrences.\n",
    "\n",
    "    Parameters:\n",
    "    - data_transactions (pd.DataFrame): DataFrame containing transaction information.\n",
    "    - data_alert_transactions (pd.DataFrame): DataFrame with alert transaction information, including SAR labels.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Merged DataFrame with transaction labels indicating SAR occurrences.\n",
    "    \"\"\"\n",
    "    transaction_labels = data_transactions[\n",
    "        [\"source\", \"target\", \"tran_id\", \"tran_timestamp\"]\n",
    "    ].merge(\n",
    "        data_alert_transactions[[\"is_sar\", \"tran_id\"]],\n",
    "        on=[\"tran_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    transaction_labels.is_sar = transaction_labels.is_sar.map({\n",
    "        True: 1,\n",
    "        np.nan: 0,\n",
    "    })\n",
    "    transaction_labels.sort_values(\n",
    "        'tran_id',\n",
    "        inplace=True,\n",
    "    )\n",
    "    transaction_labels.rename(columns={\"tran_id\": \"id\"}, inplace=True)\n",
    "    return transaction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f722dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_out_transactions(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate monthly outgoing transaction statistics for each source ID.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing transaction information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with monthly outgoing transaction statistics.\n",
    "    \"\"\"\n",
    "    out_df = data.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'source']) \\\n",
    "        .agg(monthly_count=('source', 'count'),\n",
    "             monthly_total_amount=('base_amt', 'sum'),\n",
    "             monthly_mean_amount=('base_amt', 'mean'),\n",
    "             monthly_std_amount=('base_amt', 'std')\n",
    "             )\n",
    "    out_df = out_df.reset_index(level=[\"source\"])\n",
    "    out_df = out_df.reset_index(level=[\"tran_timestamp\"])\n",
    "    out_df.columns = [\"tran_timestamp\", \"id\", \"monthly_out_count\", \"monthly_out_total_amount\",\n",
    "                      \"monthly_out_mean_amount\", \"monthly_out_std_amount\"]\n",
    "    out_df.tran_timestamp = out_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def get_in_transactions(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate monthly incoming transaction statistics for each target ID.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing transaction information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with monthly incoming transaction statistics.\n",
    "    \"\"\"\n",
    "    in_df = data.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'target']) \\\n",
    "        .agg(monthly_count=('target', 'count'),\n",
    "             monthly_total_amount=('base_amt', 'sum'),\n",
    "             monthly_mean_amount=('base_amt', 'mean'),\n",
    "             monthly_std_amount=('base_amt', 'std'))\n",
    "\n",
    "    in_df = in_df.reset_index(level=[\"target\"])\n",
    "    in_df = in_df.reset_index(level=[\"tran_timestamp\"])\n",
    "    in_df.columns = [\"tran_timestamp\", \"id\", \"monthly_in_count\", \"monthly_in_total_amount\",\n",
    "                     \"monthly_in_mean_amount\", \"monthly_in_std_amount\"]\n",
    "    in_df.tran_timestamp = in_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "    return in_df\n",
    "\n",
    "\n",
    "def get_in_out_transactions(data_transactions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge monthly incoming and outgoing transaction statistics.\n",
    "\n",
    "    Parameters:\n",
    "    - data_transactions (pd.DataFrame): DataFrame containing transaction information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Merged DataFrame with monthly incoming and outgoing transaction statistics.\n",
    "    \"\"\"\n",
    "    out_df = get_out_transactions(data_transactions)\n",
    "    in_df = get_in_transactions(data_transactions)\n",
    "    in_out_df = in_df.merge(out_df, on=['tran_timestamp', 'id'], how=\"outer\")\n",
    "    in_out_df = in_out_df.fillna(0)\n",
    "    return in_out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c354e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "# from features.transactions import get_in_out_transactions\n",
    "# from features.transaction_labels import get_transaction_labels\n",
    "# from features.party import get_party_labels\n",
    "# from features.graph_embeddings import construct_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf21dc",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üíΩ Loading the Data </span>\n",
    "\n",
    "The data you will use comes from three different CSV files:\n",
    "\n",
    "- `transactions.csv`: Transaction information such as timestamp, location, and the amount. \n",
    "- `alert_transactions.csv`: Suspicious Activity Report (SAR) transactions.\n",
    "- `party.csv`: User profile information.\n",
    "\n",
    "In a production system, these CSV files would originate from separate data sources or tables, and probably separate data pipelines. **All three files have a customer id column `id` in common, which we can use for joins.**\n",
    "\n",
    "Let's go ahead and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d42f26",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ‚õ≥Ô∏è Transactions dataset </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045cfb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tran_id</th>\n",
       "      <th>tx_type</th>\n",
       "      <th>base_amt</th>\n",
       "      <th>tran_timestamp</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>496</td>\n",
       "      <td>TRANSFER-FanOut</td>\n",
       "      <td>858.77</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>3aa9646b</td>\n",
       "      <td>1e46e726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1342</td>\n",
       "      <td>TRANSFER-Mutual</td>\n",
       "      <td>386.86</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>49203bc3</td>\n",
       "      <td>a74d1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1580</td>\n",
       "      <td>TRANSFER-FanOut</td>\n",
       "      <td>616.43</td>\n",
       "      <td>2020-01-02 00:00:00+00:00</td>\n",
       "      <td>616d4505</td>\n",
       "      <td>99af2455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tran_id          tx_type  base_amt            tran_timestamp       src  \\\n",
       "0      496  TRANSFER-FanOut    858.77 2020-01-01 00:00:00+00:00  3aa9646b   \n",
       "1     1342  TRANSFER-Mutual    386.86 2020-01-01 00:00:00+00:00  49203bc3   \n",
       "2     1580  TRANSFER-FanOut    616.43 2020-01-02 00:00:00+00:00  616d4505   \n",
       "\n",
       "        dst  \n",
       "0  1e46e726  \n",
       "1  a74d1101  \n",
       "2  99af2455  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df = pd.read_csv(\n",
    "    \"https://repo.hops.works/master/hopsworks-tutorials/data/aml/transactions.csv\", \n",
    "    parse_dates = ['tran_timestamp'],\n",
    ")\n",
    "transactions_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73630c31",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ‚õ≥Ô∏è Alert Transactions dataset </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c988da62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_id</th>\n",
       "      <th>alert_type</th>\n",
       "      <th>is_sar</th>\n",
       "      <th>tran_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>gather_scatter</td>\n",
       "      <td>True</td>\n",
       "      <td>11873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>gather_scatter</td>\n",
       "      <td>True</td>\n",
       "      <td>11874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>gather_scatter</td>\n",
       "      <td>True</td>\n",
       "      <td>11875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alert_id      alert_type  is_sar  tran_id\n",
       "0        47  gather_scatter    True    11873\n",
       "1        47  gather_scatter    True    11874\n",
       "2        47  gather_scatter    True    11875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alert_transactions = pd.read_csv(\n",
    "    \"https://repo.hops.works/master/hopsworks-tutorials/data/aml/alert_transactions.csv\",\n",
    ")\n",
    "alert_transactions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b44fd",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ‚õ≥Ô∏è Party dataset </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a87d8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partyId</th>\n",
       "      <th>partyType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5628bd6c</td>\n",
       "      <td>Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a1fcba39</td>\n",
       "      <td>Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f56c9501</td>\n",
       "      <td>Individual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    partyId     partyType\n",
       "0  5628bd6c  Organization\n",
       "1  a1fcba39  Organization\n",
       "2  f56c9501    Individual"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party = pd.read_csv(\n",
    "    \"https://repo.hops.works/master/hopsworks-tutorials/data/aml/party.csv\",\n",
    ")\n",
    "party.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004817b1",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üõ†Ô∏è Feature Engineering </span>\n",
    "\n",
    "To investigate patterns of suspicious activities you will make time window aggregates such monthly frequency, total, mean and standard deviation of amount of incoming and outgoing transasactions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f567a30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>tran_timestamp</th>\n",
       "      <th>tran_id</th>\n",
       "      <th>base_amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3aa9646b</td>\n",
       "      <td>1e46e726</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>496</td>\n",
       "      <td>858.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49203bc3</td>\n",
       "      <td>a74d1101</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>1342</td>\n",
       "      <td>386.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>616d4505</td>\n",
       "      <td>99af2455</td>\n",
       "      <td>2020-01-02 00:00:00+00:00</td>\n",
       "      <td>1580</td>\n",
       "      <td>616.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source    target            tran_timestamp  tran_id  base_amt\n",
       "0  3aa9646b  1e46e726 2020-01-01 00:00:00+00:00      496    858.77\n",
       "1  49203bc3  a74d1101 2020-01-01 00:00:00+00:00     1342    386.86\n",
       "2  616d4505  99af2455 2020-01-02 00:00:00+00:00     1580    616.43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming columns for clarity\n",
    "transactions_df.columns = ['tran_id', 'tx_type', 'base_amt', 'tran_timestamp', 'source', 'target']\n",
    "\n",
    "# Reordering columns for better readability\n",
    "transactions_df = transactions_df[[\"source\", \"target\", \"tran_timestamp\", \"tran_id\", \"base_amt\"]]\n",
    "\n",
    "# Displaying the first few rows of the DataFrame\n",
    "transactions_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7079b",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚õ≥Ô∏è Incoming and Outgoing transactions </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3774b0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tran_timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>monthly_in_count</th>\n",
       "      <th>monthly_in_total_amount</th>\n",
       "      <th>monthly_in_mean_amount</th>\n",
       "      <th>monthly_in_std_amount</th>\n",
       "      <th>monthly_out_count</th>\n",
       "      <th>monthly_out_total_amount</th>\n",
       "      <th>monthly_out_mean_amount</th>\n",
       "      <th>monthly_out_std_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1580428800000</td>\n",
       "      <td>0016359b</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1872.92</td>\n",
       "      <td>468.230000</td>\n",
       "      <td>175.274700</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1843.32</td>\n",
       "      <td>460.83</td>\n",
       "      <td>252.951744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1580428800000</td>\n",
       "      <td>001dcc27</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5874.64</td>\n",
       "      <td>652.737778</td>\n",
       "      <td>271.236889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1580428800000</td>\n",
       "      <td>00298665</td>\n",
       "      <td>1.0</td>\n",
       "      <td>755.64</td>\n",
       "      <td>755.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>521.11</td>\n",
       "      <td>521.11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tran_timestamp        id  monthly_in_count  monthly_in_total_amount  \\\n",
       "0   1580428800000  0016359b               4.0                  1872.92   \n",
       "1   1580428800000  001dcc27               9.0                  5874.64   \n",
       "2   1580428800000  00298665               1.0                   755.64   \n",
       "\n",
       "   monthly_in_mean_amount  monthly_in_std_amount  monthly_out_count  \\\n",
       "0              468.230000             175.274700                4.0   \n",
       "1              652.737778             271.236889                0.0   \n",
       "2              755.640000               0.000000                1.0   \n",
       "\n",
       "   monthly_out_total_amount  monthly_out_mean_amount  monthly_out_std_amount  \n",
       "0                   1843.32                   460.83              252.951744  \n",
       "1                      0.00                     0.00                0.000000  \n",
       "2                    521.11                   521.11                0.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a DataFrame with monthly incoming and outgoing transaction statistics\n",
    "in_out_df = get_in_out_transactions(transactions_df)\n",
    "\n",
    "# Displaying the first few rows of the resulting DataFrame\n",
    "in_out_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35e1e2",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ‚õ≥Ô∏è Transactions identified as suspicious activity </span>\n",
    "\n",
    "Assign labels to transactions that were identified as suspicius activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f10747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_id</th>\n",
       "      <th>alert_type</th>\n",
       "      <th>is_sar</th>\n",
       "      <th>tran_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>gather_scatter</td>\n",
       "      <td>True</td>\n",
       "      <td>11873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>gather_scatter</td>\n",
       "      <td>True</td>\n",
       "      <td>11874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>gather_scatter</td>\n",
       "      <td>True</td>\n",
       "      <td>11875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alert_id      alert_type  is_sar  tran_id\n",
       "0        47  gather_scatter    True    11873\n",
       "1        47  gather_scatter    True    11874\n",
       "2        47  gather_scatter    True    11875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first few rows of the 'alert_transactions' DataFrame\n",
    "alert_transactions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdc283f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>tran_timestamp</th>\n",
       "      <th>is_sar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>322886</th>\n",
       "      <td>cee9cf6d</td>\n",
       "      <td>79c248ae</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307052</th>\n",
       "      <td>65ab2f44</td>\n",
       "      <td>b20ce84b</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181198</th>\n",
       "      <td>2a39b731</td>\n",
       "      <td>a07edae4</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source    target  id            tran_timestamp  is_sar\n",
       "322886  cee9cf6d  79c248ae   2 2020-01-01 00:00:00+00:00       0\n",
       "307052  65ab2f44  b20ce84b   3 2020-01-01 00:00:00+00:00       0\n",
       "181198  2a39b731  a07edae4   4 2020-01-01 00:00:00+00:00       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating transaction labels based on transaction and alert transaction data\n",
    "transaction_labels = get_transaction_labels(\n",
    "    transactions_df, \n",
    "    alert_transactions,\n",
    ")\n",
    "\n",
    "# Displaying the first three rows of the resulting DataFrame\n",
    "transaction_labels.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeab13e",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ‚õ≥Ô∏è Party dataset </span>\n",
    "\n",
    "Now lets prepare profile (party) dataset and assign lables whether they have been reported for suspicius activity or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b35c01dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5628bd6c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a1fcba39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f56c9501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  type\n",
       "0  5628bd6c     1\n",
       "1  a1fcba39     1\n",
       "2  f56c9501     0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming columns for clarity\n",
    "party.columns = [\"id\", \"type\"]\n",
    "\n",
    "# Mapping 'type' values to numerical values for better representation\n",
    "party.type = party.type.map({\"Individual\": 0, \"Organization\": 1})\n",
    "\n",
    "# Displaying the first three rows of the DataFrame\n",
    "party.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeccf314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>tran_timestamp</th>\n",
       "      <th>is_sar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41322</th>\n",
       "      <td>5e7442f1</td>\n",
       "      <td>0bffd1da</td>\n",
       "      <td>11873</td>\n",
       "      <td>2020-01-09 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62128</th>\n",
       "      <td>65c7b5a1</td>\n",
       "      <td>0bffd1da</td>\n",
       "      <td>11874</td>\n",
       "      <td>2020-01-09 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57575</th>\n",
       "      <td>04128f28</td>\n",
       "      <td>0bffd1da</td>\n",
       "      <td>11875</td>\n",
       "      <td>2020-01-09 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         source    target     id            tran_timestamp  is_sar\n",
       "41322  5e7442f1  0bffd1da  11873 2020-01-09 00:00:00+00:00       1\n",
       "62128  65c7b5a1  0bffd1da  11874 2020-01-09 00:00:00+00:00       1\n",
       "57575  04128f28  0bffd1da  11875 2020-01-09 00:00:00+00:00       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering transactions with SAR(Suspicious Activity Reports) labels from the generated transaction labels DataFrame\n",
    "alert_transactions = transaction_labels[transaction_labels.is_sar == 1]\n",
    "\n",
    "# Displaying the first few rows of transactions flagged as SAR\n",
    "alert_transactions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c8a2f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/xpdsfcns3f30q6mpm2273hr80000gn/T/ipykernel_5809/3577552328.py:21: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sar_party = alert_sources.append(alert_targets, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>tran_timestamp</th>\n",
       "      <th>is_sar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5628bd6c</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a1fcba39</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f56c9501</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  type       tran_timestamp  is_sar\n",
       "0  5628bd6c     1  2021-12-20 00:00:00       0\n",
       "1  a1fcba39     1  2021-12-20 00:00:00       0\n",
       "2  f56c9501     0  2021-12-20 00:00:00       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating party labels based on transaction labels and party information\n",
    "party_labels = get_party_labels(\n",
    "    transaction_labels, \n",
    "    party,\n",
    ")\n",
    "\n",
    "# Displaying the first three rows of the resulting DataFrame\n",
    "party_labels.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7ba00",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üß¨ Graph representational learning using Graph Neural Network</span>\n",
    "\n",
    "Finanial transactions can be represented as a dynamic network graph. Using technique of graph representation \n",
    "give as opportunity to represent transaction with a broader context. In this example you will perfom node \n",
    "representation learning. \n",
    "\n",
    "Network architecture of the graph convolution layer for learning node represantion learning  was taken from \n",
    "[this Keras example](https://keras.io/examples/graph/gnn_citations/).  It performs the following steps:\n",
    "\n",
    "1. **Prepare**: The input node representations are processed using a FFN to produce a *message*. You can simplify\n",
    "the processing by only applying linear transformation to the representations.\n",
    "2. **Aggregate**: The messages of the neighbours of each node are aggregated with\n",
    "respect to the `edge_weights` using a *permutation invariant* pooling operation, such as *sum*, *mean*, and *max*,\n",
    "to prepare a single aggregated message for each node. See, for example, [tf.math.unsorted_segment_sum](https://www.tensorflow.org/api_docs/python/tf/math/unsorted_segment_sum)\n",
    "APIs used to aggregate neighbour messages.\n",
    "3. **Update**: The `node_repesentations` and `aggregated_messages`‚Äîboth of shape `[num_nodes, representation_dim]`‚Äî\n",
    "are combined and processed to produce the new state of the node representations (node embeddings).\n",
    "If `combination_type` is `gru`, the `node_repesentations` and `aggregated_messages` are stacked to create a sequence,\n",
    "then processed by a GRU layer. Otherwise, the `node_repesentations` and `aggregated_messages` are added\n",
    "or concatenated, then processed using a FFN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b6de9",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">üîÆ Compute time evolving graph embeddings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5f565ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 16:37:45.972169: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 114ms/step - loss: 17083342.0000 - acc: 1.3965e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 116ms/step - loss: 17082572.0000 - acc: 1.3965e-04\n",
      "224/224 [==============================] - 4s 14ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 114ms/step - loss: 16765117.0000 - acc: 1.4096e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 115ms/step - loss: 16764311.0000 - acc: 1.4096e-04\n",
      "222/222 [==============================] - 3s 13ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 122ms/step - loss: 16930882.0000 - acc: 1.4027e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 141ms/step - loss: 16930192.0000 - acc: 4.2082e-04\n",
      "223/223 [==============================] - 5s 19ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 120ms/step - loss: 16959588.0000 - acc: 1.4015e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 121ms/step - loss: 16958712.0000 - acc: 1.4015e-04\n",
      "223/223 [==============================] - 4s 15ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 124ms/step - loss: 16983252.0000 - acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 127ms/step - loss: 16982496.0000 - acc: 4.2017e-04\n",
      "224/224 [==============================] - 4s 15ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 115ms/step - loss: 16921508.0000 - acc: 2.8062e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 115ms/step - loss: 16920692.0000 - acc: 2.8062e-04\n",
      "223/223 [==============================] - 4s 13ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 121ms/step - loss: 16859666.0000 - acc: 2.8114e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 120ms/step - loss: 16859000.0000 - acc: 2.8114e-04\n",
      "223/223 [==============================] - 4s 16ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 127ms/step - loss: 16983276.0000 - acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 124ms/step - loss: 16982494.0000 - acc: 1.4006e-04\n",
      "224/224 [==============================] - 4s 16ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 119ms/step - loss: 17059366.0000 - acc: 2.7949e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 118ms/step - loss: 17058718.0000 - acc: 1.3974e-04\n",
      "224/224 [==============================] - 3s 14ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 123ms/step - loss: 16888148.0000 - acc: 1.4045e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 120ms/step - loss: 16887460.0000 - acc: 0.0000e+00\n",
      "223/223 [==============================] - 3s 14ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 134ms/step - loss: 16869230.0000 - acc: 1.4053e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 16868484.0000 - acc: 0.0000e+00\n",
      "223/223 [==============================] - 4s 14ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 121ms/step - loss: 17073830.0000 - acc: 5.5874e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 121ms/step - loss: 17073028.0000 - acc: 1.3968e-04\n",
      "224/224 [==============================] - 4s 14ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 121ms/step - loss: 16859656.0000 - acc: 1.4057e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 121ms/step - loss: 16859002.0000 - acc: 0.0000e+00\n",
      "223/223 [==============================] - 4s 14ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 115ms/step - loss: 16764960.0000 - acc: 1.4096e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 3s 114ms/step - loss: 16764309.0000 - acc: 0.0000e+00\n",
      "222/222 [==============================] - 3s 13ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 8s 123ms/step - loss: 16973796.0000 - acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 127ms/step - loss: 16972984.0000 - acc: 0.0000e+00\n",
      "224/224 [==============================] - 4s 15ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 159ms/step - loss: 17006958.0000 - acc: 2.7992e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 146ms/step - loss: 17006298.0000 - acc: 1.3996e-04\n",
      "224/224 [==============================] - 5s 21ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 10s 158ms/step - loss: 17049772.0000 - acc: 2.7956e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 146ms/step - loss: 17049182.0000 - acc: 2.7956e-04\n",
      "224/224 [==============================] - 6s 23ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 11s 153ms/step - loss: 16802886.0000 - acc: 2.8161e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 134ms/step - loss: 16802158.0000 - acc: 2.8161e-04\n",
      "222/222 [==============================] - 5s 21ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 131ms/step - loss: 17059354.0000 - acc: 2.7949e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 128ms/step - loss: 17058720.0000 - acc: 1.3974e-04\n",
      "224/224 [==============================] - 4s 17ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 129ms/step - loss: 17073762.0000 - acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 130ms/step - loss: 17073028.0000 - acc: 1.3968e-04\n",
      "224/224 [==============================] - 4s 18ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 124ms/step - loss: 16732007.0000 - acc: 2.8221e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 127ms/step - loss: 16731231.0000 - acc: 0.0000e+00\n",
      "222/222 [==============================] - 4s 17ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 131ms/step - loss: 16978546.0000 - acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 128ms/step - loss: 16977736.0000 - acc: 1.4008e-04\n",
      "224/224 [==============================] - 4s 17ms/step\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 9s 128ms/step - loss: 17016444.0000 - acc: 1.3992e-04\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 4s 129ms/step - loss: 17015822.0000 - acc: 1.3992e-04\n",
      "224/224 [==============================] - 4s 17ms/step\n",
      "Epoch 1/2\n",
      "27/27 [==============================] - 8s 103ms/step - loss: 15631237.0000 - acc: 1.4599e-04\n",
      "Epoch 2/2\n",
      "27/27 [==============================] - 3s 102ms/step - loss: 15630564.0000 - acc: 0.0000e+00\n",
      "215/215 [==============================] - 3s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# Grouping transaction labels by month using pandas Grouper\n",
    "transaction_graphs_by_month = transaction_labels.groupby(\n",
    "    pd.Grouper(key='tran_timestamp', freq='M')\n",
    ").apply(lambda x: construct_graph(x, party_labels))\n",
    "\n",
    "# The resulting variable 'transaction_graphs_by_month' is a pandas DataFrame\n",
    "# where each row corresponds to a month, and the 'graph_embeddings' column contains\n",
    "# the node embeddings generated for each month using the 'construct_graph' function.\n",
    "# The embeddings capture the graph structure of transactions during that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25d4fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting timestamps and graph embeddings\n",
    "timestamps = transaction_graphs_by_month.index.values\n",
    "graph_embeddings = transaction_graphs_by_month.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4bdcb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>graph_embeddings</th>\n",
       "      <th>tran_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5628bd6c</td>\n",
       "      <td>[0.99970543, 0.9996497, 0.9996238, 0.99984884,...</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a1fcba39</td>\n",
       "      <td>[0.9997054, 0.99964976, 0.9996239, 0.99984896,...</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f56c9501</td>\n",
       "      <td>[0.9997054, 0.99964976, 0.9996239, 0.99984884,...</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                   graph_embeddings tran_timestamp\n",
       "0  5628bd6c  [0.99970543, 0.9996497, 0.9996238, 0.99984884,...     2020-01-31\n",
       "1  a1fcba39  [0.9997054, 0.99964976, 0.9996239, 0.99984896,...     2020-01-31\n",
       "2  f56c9501  [0.9997054, 0.99964976, 0.9996239, 0.99984884,...     2020-01-31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an empty DataFrame to store graph embeddings\n",
    "graph_embeddings_df = pd.DataFrame()\n",
    "\n",
    "# Iterating through timestamps and corresponding graph embeddings\n",
    "for timestamp, graph_embedding in zip(timestamps, graph_embeddings):\n",
    "    # Creating a temporary DataFrame for each month's graph embeddings\n",
    "    df_tmp = pd.DataFrame(graph_embedding)\n",
    "    \n",
    "    # Adding a 'tran_timestamp' column to store the timestamp for each row\n",
    "    df_tmp[\"tran_timestamp\"] = timestamp\n",
    "    \n",
    "    # Concatenating the temporary DataFrame to the main DataFrame\n",
    "    graph_embeddings_df = pd.concat([graph_embeddings_df, df_tmp])\n",
    "\n",
    "# Displaying the first three rows of the resulting DataFrame\n",
    "graph_embeddings_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b6eb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'tran_timestamp' values to milliseconds for consistency\n",
    "transaction_labels.tran_timestamp = transaction_labels.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "graph_embeddings_df.tran_timestamp = graph_embeddings_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "\n",
    "# Converting 'tran_timestamp' values in 'party_labels' to milliseconds\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.map(lambda x: datetime.datetime.timestamp(x) * 1000)\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa90ac4",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üëÆüèª‚Äç‚ôÇÔ∏è Data Validation</span>\n",
    "\n",
    "Before you define [feature groups](https://docs.hopsworks.ai/latest/generated/feature_group/) lets define [validation rules](https://docs.hopsworks.ai/latest/generated/feature_validation/) for features. You do expect some of the features to comply with certain *rules* or *expectations*. For example: a transacted amount must be a positive value. In the case of a transacted amount arriving as a negative value you can decide whether to stop it to `write` into a feature group and throw an error or allow it to be written but provide a warning. In the next section you will create feature store `expectations`, attach them to feature groups, and apply them to dataframes being appended to said feature group.\n",
    "\n",
    "#### Data validation with Greate Expectations in Hopsworks\n",
    "You can use GE library for validation in Hopsworks features store. \n",
    "\n",
    "##  <img src=\"../images/icon102.png\" width=\"18px\"></img> Hopsworks feature store\n",
    "\n",
    "The Hopsworks feature feature store library is Apache V2 licensed and available [here](https://github.com/logicalclocks/feature-store-api). The library is currently available for Python and JVM languages such as Scala and Java.\n",
    "In this notebook, we are going to cover Python part.\n",
    "\n",
    "You can find the complete documentation of the library here: \n",
    "\n",
    "The first step is to establish a connection with your Hopsworks feature store instance and retrieve the object that represents the feature store you'll be working with. \n",
    "\n",
    "> By default `project.get_feature_store()` returns the feature store of the project we are working with. However, it accepts also a project name as parameter to select a different feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "368b3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dab9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxzhytnikov/opt/anaconda3/envs/aml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/282773\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe70da0",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üî¨ Expectations suite</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45ca1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d8d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'data_asset_type': None,\n",
      "  'expectation_suite_name': 'aml_project_validations',\n",
      "  'expectations': [],\n",
      "  'ge_cloud_id': None,\n",
      "  'meta': {'great_expectations_version': '0.14.13'}}\n"
     ]
    }
   ],
   "source": [
    "# Creating an Expectation Suite named \"aml_project_validations\"\n",
    "expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aml_project_validations\",\n",
    ")\n",
    "\n",
    "# Displaying the JSON representation of the Expectation Suite\n",
    "pprint(expectation_suite.to_json_dict(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e405511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'data_asset_type': None,\n",
      "  'expectation_suite_name': 'aml_project_validations',\n",
      "  'expectations': [ { 'expectation_type': 'expect_column_max_to_be_between',\n",
      "                      'kwargs': { 'column': 'monthly_in_count',\n",
      "                                  'max_value': 10000000,\n",
      "                                  'min_value': 0},\n",
      "                      'meta': {}}],\n",
      "  'ge_cloud_id': None,\n",
      "  'meta': {'great_expectations_version': '0.14.13'}}\n"
     ]
    }
   ],
   "source": [
    "# Adding an expectation to the Expectation Suite\n",
    "expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_max_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"monthly_in_count\", \n",
    "            \"min_value\": 0, \n",
    "            \"max_value\": 10000000,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Displaying the updated Expectation Suite\n",
    "pprint(expectation_suite.to_json_dict(), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2624d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Feature Groups Creation</span>\n",
    "\n",
    "### Feature Groups\n",
    "\n",
    "A `Feature Groups` is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The `Feature Group` lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them.\n",
    "\n",
    "Generally, the features in a feature group are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, `feature groups` provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in.\n",
    "\n",
    "> It is important to note that `feature groups` are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7c3ef",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚õ≥Ô∏è Transactions monthly aggregates Feature Group</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c11649a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: Providing event_time as a single-element list is deprecated and will be dropped in future versions. Provide the feature_name string instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/282773/fs/282692/fg/309629\n",
      "2023-12-12 16:44:29,093 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/282773/fs/282692/fg/309629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Uploading Dataframe: 0.00% | | Rows 0/170876 | Elapsed Time: 00:00 | Remaining T"
     ]
    },
    {
     "ename": "KafkaException",
     "evalue": "KafkaError{code=TOPIC_AUTHORIZATION_FAILED,val=29,str=\"Unable to produce message: Broker: Topic authorization failed\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKafkaException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 20\u001b[0m\n\u001b[1;32m      2\u001b[0m transactions_fg \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mget_or_create_feature_group(\n\u001b[1;32m      3\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransactions_monthly\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     expectation_suite\u001b[38;5;241m=\u001b[39mexpectation_suite,\n\u001b[1;32m     18\u001b[0m )   \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Insert data into the feature group\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtransactions_fg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_out_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aml/lib/python3.10/site-packages/hsfs/feature_group.py:1984\u001b[0m, in \u001b[0;36mFeatureGroup.insert\u001b[0;34m(self, features, overwrite, operation, storage, write_options, validation_options, save_code, wait)\u001b[0m\n\u001b[1;32m   1981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_for_job\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m write_options:\n\u001b[1;32m   1982\u001b[0m     write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_for_job\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m wait\n\u001b[0;32m-> 1984\u001b[0m job, ge_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1985\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_code \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   1994\u001b[0m     ge_report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ge_report\u001b[38;5;241m.\u001b[39mingestion_result \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINGESTED\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1995\u001b[0m ):\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_engine\u001b[38;5;241m.\u001b[39msave_code(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aml/lib/python3.10/site-packages/hsfs/core/feature_group_engine.py:124\u001b[0m, in \u001b[0;36mFeatureGroupEngine.insert\u001b[0;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_group_api\u001b[38;5;241m.\u001b[39mdelete_content(feature_group)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 124\u001b[0m     \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbulk_insert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    133\u001b[0m     ge_report,\n\u001b[1;32m    134\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aml/lib/python3.10/site-packages/hsfs/engine/python.py:495\u001b[0m, in \u001b[0;36mEngine.save_dataframe\u001b[0;34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_dataframe\u001b[39m(\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    482\u001b[0m     feature_group: FeatureGroup,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     validation_id: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    490\u001b[0m ):\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(feature_group, ExternalFeatureGroup)\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39monline_enabled\n\u001b[1;32m    494\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m--> 495\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_dataframe_kafka\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_save_dataframe(\n\u001b[1;32m    501\u001b[0m             feature_group,\n\u001b[1;32m    502\u001b[0m             dataframe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m             validation_id,\n\u001b[1;32m    509\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aml/lib/python3.10/site-packages/hsfs/engine/python.py:1006\u001b[0m, in \u001b[0;36mEngine._write_dataframe_kafka\u001b[0;34m(self, feature_group, dataframe, offline_write_options)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;66;03m# assemble key\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(row[pk]) \u001b[38;5;28;01mfor\u001b[39;00m pk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(feature_group\u001b[38;5;241m.\u001b[39mprimary_key)])\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kafka_produce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproducer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# make sure producer blocks and everything is delivered\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aml/lib/python3.10/site-packages/hsfs/engine/python.py:1084\u001b[0m, in \u001b[0;36mEngine._kafka_produce\u001b[0;34m(self, producer, feature_group, key, encoded_row, acked, offline_write_options)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;66;03m# if BufferError is thrown, we can be sure, message hasn't been send so we retry\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;66;03m# produce\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m         \u001b[43mproducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_online_topic_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprojectId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatureGroupId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubjectId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubject\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;66;03m# Trigger internal callbacks to empty op queue\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m         producer\u001b[38;5;241m.\u001b[39mpoll(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKafkaException\u001b[0m: KafkaError{code=TOPIC_AUTHORIZATION_FAILED,val=29,str=\"Unable to produce message: Broker: Topic authorization failed\"}"
     ]
    }
   ],
   "source": [
    "# Get or create the 'transactions_monthly' feature group\n",
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name=\"transactions_monthly\",\n",
    "    version=1,\n",
    "    primary_key=[\"id\"],\n",
    "    partition_key=[\"tran_timestamp\"],   \n",
    "    description=\"transactions monthly aggregates features\",\n",
    "    event_time=['tran_timestamp'],\n",
    "    online_enabled=True,\n",
    "    stream=True,\n",
    "    statistics_config={\n",
    "        \"enabled\": True, \n",
    "        \"histograms\": True, \n",
    "        \"correlations\": True, \n",
    "        \"exact_uniqueness\": False,\n",
    "    },\n",
    "    expectation_suite=expectation_suite,\n",
    ")   \n",
    "# Insert data into the feature group\n",
    "transactions_fg.insert(in_out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68377a75",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚õ≥Ô∏è Party Feature Group</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca78342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create the 'party_labels' feature group\n",
    "party_fg = fs.get_or_create_feature_group(\n",
    "    name = \"party_labels\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    description = \"party fg with labels\",\n",
    "    event_time = ['tran_timestamp'],        \n",
    "    online_enabled = True,\n",
    "    stream=True,\n",
    "    statistics_config = {\n",
    "        \"enabled\": True, \n",
    "        \"histograms\": True, \n",
    "        \"correlations\": True, \n",
    "        \"exact_uniqueness\": False,\n",
    "    },\n",
    ")\n",
    "# Insert data into the feature group\n",
    "party_fg.insert(party_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda4e5a",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚õ≥Ô∏è Graph embeddings Feature Group</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0adde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs import engine\n",
    "features = engine.get_instance().parse_schema_feature_group(graph_embeddings_df)\n",
    "for f in features:\n",
    "    if f.type == \"array<float>\":\n",
    "        f.online_type = \"VARBINARY(200)\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1702401702.609|FAIL|rdkafka#producer-1| [thrd:ssl://3.138.125.47:9092/bootstrap]: ssl://3.138.125.47:9092/2: Disconnected (after 9431431ms in state UP)\n"
     ]
    }
   ],
   "source": [
    "# Get or create the 'graph_embeddings' feature group\n",
    "graph_embeddings_fg = fs.get_or_create_feature_group(\n",
    "    name=\"graph_embeddings\",\n",
    "    version=1,\n",
    "    primary_key=[\"id\"],\n",
    "    description=\"node embeddings from transactions graph\",\n",
    "    event_time = ['tran_timestamp'],      \n",
    "    online_enabled=True,       \n",
    "    stream=True,\n",
    "    statistics_config={\n",
    "        \"enabled\": False, \n",
    "        \"histograms\": False, \n",
    "        \"correlations\": False, \n",
    "        \"exact_uniqueness\": False,\n",
    "    },\n",
    "    features=features,\n",
    ")\n",
    "# Insert data into the feature group\n",
    "graph_embeddings_fg.insert(graph_embeddings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dcbe1",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> üëì Exploration </span>\n",
    "\n",
    "### Feature groups are now accessible and searchable in the UI\n",
    "![fg-overview](images/fg_explore.gif)\n",
    "\n",
    "## üìä Statistics\n",
    "We can explore feature statistics in the feature groups. If statistics was not enabled when feature group was created then this can be done by:\n",
    "\n",
    "```python\n",
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transactions_monthly_fg\", \n",
    "    version = 1)\n",
    "\n",
    "transactions_fg.statistics_config = {\n",
    "    \"enabled\": True,\n",
    "    \"histograms\": True,\n",
    "    \"correlations\": True\n",
    "}\n",
    "\n",
    "transactions_fg.update_statistics_config()\n",
    "transactions_fg.compute_statistics()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603f1a0",
   "metadata": {},
   "source": [
    "![fg-stats](images/freature_group_stats.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e2ad32",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> ‚è≠Ô∏è **Next:** Part 02 </span>\n",
    "    \n",
    "In the next notebook you will create a training dataset, train and deploy a trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
